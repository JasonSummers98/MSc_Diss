{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sarcasm Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import nps_chat\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import pandas as pd \n",
    "import re\n",
    "import pickle\n",
    "import datetime\n",
    "import io\n",
    "\n",
    "\n",
    "import os\n",
    "from os import makedirs\n",
    "from os import chdir\n",
    "from os import path\n",
    "cwd = os.getcwd()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENdata = pd.read_csv(cwd + '/sarcasmCorpora/GEN-sarc-notsarc.csv')\n",
    "HYPdata = pd.read_csv(cwd + '/sarcasmCorpora/HYP-sarc-notsarc.csv')\n",
    "RQdata = pd.read_csv(cwd + '/sarcasmCorpora/RQ-sarc-notsarc.csv')\n",
    "\n",
    "GENdata['Type'] = 'general' \n",
    "HYPdata['Type'] = 'hyperbole'\n",
    "RQdata['Type'] = 'rhetorical'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data splitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(text, labels, split_size):\n",
    "    sentences_train, sentences_test, label_train, label_test = train_test_split(\n",
    "        text, labels, test_size = split_size, random_state = 42)\n",
    "    \n",
    "    sentences_train = np.array(sentences_train)\n",
    "    label_train = np.array(label_train)\n",
    "    sentences_test = np.array(sentences_test)\n",
    "    label_test = np.array(label_test)\n",
    "    \n",
    "    return sentences_train, sentences_test, label_train, label_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding Sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 200\n",
    "\n",
    "def make_sequences(tok, train_text, test_text):\n",
    "    training_sequences = tok.texts_to_sequences(train_text)\n",
    "    training_padded = pad_sequences(training_sequences, maxlen=max_length, \n",
    "                                    padding = 'post', truncating = 'post')\n",
    "    \n",
    "    testing_sequences = tok.texts_to_sequences(test_text)\n",
    "    testing_padded = pad_sequences(testing_sequences, maxlen=max_length, \n",
    "                                   padding = 'post', truncating = 'post')\n",
    "    \n",
    "    return training_padded, testing_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"\\'s\", \" is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"\\?\", \"\", text)\n",
    "    text = re.sub(r\"\\!\", \"\", text)\n",
    "    text = re.sub(r\"\\.\", \"\", text)\n",
    "    text = re.sub(r\"\\,\", \"\", text)\n",
    "    text = re.sub(r\"\\r\\n\", \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN1: Type of input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPdata['id'] = HYPdata['id'].apply(lambda x: x + 6520)\n",
    "RQdata['id'] = RQdata['id'].apply(lambda x: x + 6520 + 1164)\n",
    "\n",
    "sarcasm_dataset = GENdata.append(HYPdata).append(RQdata).set_index('id')\n",
    "\n",
    "type_labels = sarcasm_dataset['Type'].tolist()\n",
    "sentencesRaw = sarcasm_dataset['text'].tolist()\n",
    "sentences = list(map(clean_text, sentencesRaw))\n",
    "\n",
    "Type_test_size = 0.5\n",
    "\n",
    "Type_train, Type_test, Type_label_train, Type_label_test = split_data(sentences, type_labels, Type_test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl_Type_train, nl_Type_test = [],[]\n",
    "\n",
    "def number_label(l, new_l):\n",
    "    for element in l:\n",
    "        if(element == 'general'):\n",
    "            new_l.append(0)\n",
    "        elif(element == 'hyperbole'):\n",
    "            new_l.append(1)\n",
    "        elif(element == 'rhetorical'):\n",
    "            new_l.append(2)\n",
    "            \n",
    "number_label(Type_label_train, nl_Type_train)\n",
    "number_label(Type_label_test, nl_Type_test)\n",
    "\n",
    "nl_Type_train, nl_Type_test = np.array(nl_Type_train), np.array(nl_Type_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Type_tok = Tokenizer(oov_token=\"<OOV>\")\n",
    "Type_tok.fit_on_texts(Type_train)\n",
    "Type_word_index = Type_tok.word_index\n",
    "Type_vocab_size = len(Type_word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Type_train_padded, Type_test_padded = make_sequences(Type_tok, Type_train, Type_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Type_train_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Type_embedding_dim = 32\n",
    "\n",
    "Type_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(Type_vocab_size, Type_embedding_dim, input_length = max_length, name = 'TYPEembed'),\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        tf.keras.layers.Dense(24, activation = 'relu'),\n",
    "        tf.keras.layers.Dense(3, activation = 'sigmoid'),\n",
    "    ])\n",
    "\n",
    "Type_model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "Type_model.summary()\n",
    "\n",
    "Type_history = Type_model.fit(Type_train_padded, nl_Type_train, epochs = 30, \n",
    "                              validation_data = (Type_test_padded, nl_Type_test), verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN2: Sarcasm or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEN_text = list(map(clean_text, GENdata['text'].tolist()))\n",
    "GEN_labels = GENdata['class'].tolist()\n",
    "HYP_text = list(map(clean_text, HYPdata['text'].tolist()))\n",
    "HYP_labels = HYPdata['class'].tolist()\n",
    "RQ_text = list(map(clean_text, RQdata['text'].tolist()))\n",
    "RQ_labels = RQdata['class'].tolist()\n",
    "\n",
    "GEN_test_size, HYP_test_size, RQ_test_size = 0.33, 0.1, 0.4\n",
    "\n",
    "GEN_train, GEN_test, GEN_label_train, GEN_label_test = split_data(GEN_text, GEN_labels, GEN_test_size)\n",
    "HYP_train, HYP_test, HYP_label_train, HYP_label_test = split_data(HYP_text, HYP_labels, HYP_test_size)\n",
    "RQ_train, RQ_test, RQ_label_train, RQ_label_test = split_data(RQ_text, RQ_labels, RQ_test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "blGEN_train, blGEN_test, blHYP_train, blHYP_test, blRQ_train, blRQ_test = [],[],[],[],[],[]\n",
    "\n",
    "def make_binery(l, new_l):\n",
    "    for element in l:\n",
    "        if(element == 'notsarc'):\n",
    "            new_l.append(0)\n",
    "        elif(element == 'sarc'):\n",
    "            new_l.append(1)\n",
    "            \n",
    "    new_l = np.array(new_l)\n",
    "            \n",
    "make_binery(GEN_label_train, blGEN_train)\n",
    "make_binery(GEN_label_test, blGEN_test)\n",
    "make_binery(HYP_label_train, blHYP_train)\n",
    "make_binery(HYP_label_test, blHYP_test)\n",
    "make_binery(RQ_label_train, blRQ_train)\n",
    "make_binery(RQ_label_test, blRQ_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "blGEN_train, blGEN_test = np.array(blGEN_train), np.array(blGEN_test)\n",
    "blHYP_train, blHYP_test = np.array(blHYP_train), np.array(blHYP_test)\n",
    "blRQ_train, blRQ_test = np.array(blRQ_train), np.array(blRQ_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENtok = Tokenizer(oov_token=\"<OOV>\")\n",
    "GENtok.fit_on_texts(GEN_train)\n",
    "GEN_word_index = GENtok.word_index\n",
    "GEN_vocab_size = len(GEN_word_index) + 1\n",
    "\n",
    "HYPtok = Tokenizer(oov_token=\"<OOV>\")\n",
    "HYPtok.fit_on_texts(HYP_train)\n",
    "HYP_word_index = HYPtok.word_index\n",
    "HYP_vocab_size = len(HYP_word_index) + 1\n",
    "\n",
    "RQtok = Tokenizer(oov_token=\"<OOV>\")\n",
    "RQtok.fit_on_texts(RQ_train)\n",
    "RQ_word_index = RQtok.word_index\n",
    "RQ_vocab_size = len(RQ_word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEN_train_padded, GEN_test_padded = make_sequences(GENtok, GEN_train, GEN_test)\n",
    "HYP_train_padded, HYP_test_padded = make_sequences(HYPtok, HYP_train, HYP_test)\n",
    "RQ_train_padded, RQ_test_padded = make_sequences(RQtok, RQ_train, RQ_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General NN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "GENembed (Embedding)         (None, 200, 16)           259760    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_19  (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 24)                408       \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 32)                800       \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 12)                396       \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 261,649\n",
      "Trainable params: 261,649\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "137/137 - 1s - loss: 0.6981 - accuracy: 0.5094 - val_loss: 0.6927 - val_accuracy: 0.5000\n",
      "Epoch 2/15\n",
      "137/137 - 1s - loss: 0.6931 - accuracy: 0.4982 - val_loss: 0.6911 - val_accuracy: 0.5000\n",
      "Epoch 3/15\n",
      "137/137 - 1s - loss: 0.6862 - accuracy: 0.5598 - val_loss: 0.6742 - val_accuracy: 0.6134\n",
      "Epoch 4/15\n",
      "137/137 - 1s - loss: 0.6483 - accuracy: 0.6399 - val_loss: 0.6287 - val_accuracy: 0.6571\n",
      "Epoch 5/15\n",
      "137/137 - 1s - loss: 0.6199 - accuracy: 0.6648 - val_loss: 0.6139 - val_accuracy: 0.6622\n",
      "Epoch 6/15\n",
      "137/137 - 1s - loss: 0.5986 - accuracy: 0.6854 - val_loss: 0.6060 - val_accuracy: 0.6673\n",
      "Epoch 7/15\n",
      "137/137 - 1s - loss: 0.5690 - accuracy: 0.7136 - val_loss: 0.5813 - val_accuracy: 0.6980\n",
      "Epoch 8/15\n",
      "137/137 - 1s - loss: 0.5265 - accuracy: 0.7424 - val_loss: 0.5756 - val_accuracy: 0.7198\n",
      "Epoch 9/15\n",
      "137/137 - 1s - loss: 0.4706 - accuracy: 0.7848 - val_loss: 0.5499 - val_accuracy: 0.7328\n",
      "Epoch 10/15\n",
      "137/137 - 1s - loss: 0.4111 - accuracy: 0.8217 - val_loss: 0.5486 - val_accuracy: 0.7388\n",
      "Epoch 11/15\n",
      "137/137 - 1s - loss: 0.3640 - accuracy: 0.8443 - val_loss: 0.5782 - val_accuracy: 0.7333\n",
      "Epoch 12/15\n",
      "137/137 - 1s - loss: 0.3172 - accuracy: 0.8736 - val_loss: 0.5810 - val_accuracy: 0.7337\n",
      "Epoch 13/15\n",
      "137/137 - 1s - loss: 0.2667 - accuracy: 0.8963 - val_loss: 0.6149 - val_accuracy: 0.7258\n",
      "Epoch 14/15\n",
      "137/137 - 1s - loss: 0.2335 - accuracy: 0.9098 - val_loss: 0.6377 - val_accuracy: 0.7300\n",
      "Epoch 15/15\n",
      "137/137 - 1s - loss: 0.1953 - accuracy: 0.9299 - val_loss: 0.6750 - val_accuracy: 0.7277\n"
     ]
    }
   ],
   "source": [
    "GEN_embedding_dim = 16\n",
    "\n",
    "GEN_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(GEN_vocab_size, GEN_embedding_dim, \n",
    "                                  input_length = max_length, name = 'GENembed'),\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        tf.keras.layers.Dense(16, activation = 'sigmoid'),\n",
    "        tf.keras.layers.Dense(24, activation = 'relu'),\n",
    "        tf.keras.layers.Dense(32, activation = 'sigmoid'),\n",
    "        tf.keras.layers.Dense(12, activation = 'relu'),\n",
    "        tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "    ])\n",
    "\n",
    "GEN_model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "GEN_model.summary()\n",
    "GEN_n_epochs = 15\n",
    "\n",
    "GEN_history = GEN_model.fit(GEN_train_padded, blGEN_train, epochs = GEN_n_epochs, \n",
    "                            validation_data = (GEN_test_padded, blGEN_test), verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperbolic NN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "HYPembed (Embedding)         (None, 200, 32)           250528    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_16  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 251,345\n",
      "Trainable params: 251,345\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "33/33 - 0s - loss: 0.7269 - accuracy: 0.5215 - val_loss: 0.7226 - val_accuracy: 0.5128\n",
      "Epoch 2/20\n",
      "33/33 - 0s - loss: 0.7188 - accuracy: 0.5387 - val_loss: 0.7166 - val_accuracy: 0.4444\n",
      "Epoch 3/20\n",
      "33/33 - 0s - loss: 0.7123 - accuracy: 0.5358 - val_loss: 0.7108 - val_accuracy: 0.5214\n",
      "Epoch 4/20\n",
      "33/33 - 0s - loss: 0.7058 - accuracy: 0.5922 - val_loss: 0.7062 - val_accuracy: 0.5470\n",
      "Epoch 5/20\n",
      "33/33 - 0s - loss: 0.6998 - accuracy: 0.6418 - val_loss: 0.7013 - val_accuracy: 0.5983\n",
      "Epoch 6/20\n",
      "33/33 - 0s - loss: 0.6916 - accuracy: 0.6705 - val_loss: 0.6936 - val_accuracy: 0.5470\n",
      "Epoch 7/20\n",
      "33/33 - 0s - loss: 0.6778 - accuracy: 0.6944 - val_loss: 0.6873 - val_accuracy: 0.6752\n",
      "Epoch 8/20\n",
      "33/33 - 0s - loss: 0.6507 - accuracy: 0.7612 - val_loss: 0.6748 - val_accuracy: 0.6752\n",
      "Epoch 9/20\n",
      "33/33 - 0s - loss: 0.5984 - accuracy: 0.8300 - val_loss: 0.6536 - val_accuracy: 0.6154\n",
      "Epoch 10/20\n",
      "33/33 - 0s - loss: 0.5242 - accuracy: 0.8902 - val_loss: 0.6380 - val_accuracy: 0.6496\n",
      "Epoch 11/20\n",
      "33/33 - 0s - loss: 0.4489 - accuracy: 0.8825 - val_loss: 0.6580 - val_accuracy: 0.6752\n",
      "Epoch 12/20\n",
      "33/33 - 0s - loss: 0.3764 - accuracy: 0.9131 - val_loss: 0.6303 - val_accuracy: 0.6752\n",
      "Epoch 13/20\n",
      "33/33 - 0s - loss: 0.3138 - accuracy: 0.9417 - val_loss: 0.6367 - val_accuracy: 0.6410\n",
      "Epoch 14/20\n",
      "33/33 - 0s - loss: 0.2688 - accuracy: 0.9503 - val_loss: 0.6507 - val_accuracy: 0.6496\n",
      "Epoch 15/20\n",
      "33/33 - 0s - loss: 0.2391 - accuracy: 0.9532 - val_loss: 0.6725 - val_accuracy: 0.6239\n",
      "Epoch 16/20\n",
      "33/33 - 0s - loss: 0.2123 - accuracy: 0.9656 - val_loss: 0.6861 - val_accuracy: 0.6496\n",
      "Epoch 17/20\n",
      "33/33 - 0s - loss: 0.1903 - accuracy: 0.9733 - val_loss: 0.7172 - val_accuracy: 0.6410\n",
      "Epoch 18/20\n",
      "33/33 - 0s - loss: 0.1730 - accuracy: 0.9809 - val_loss: 0.7258 - val_accuracy: 0.6410\n",
      "Epoch 19/20\n",
      "33/33 - 0s - loss: 0.1634 - accuracy: 0.9866 - val_loss: 0.7563 - val_accuracy: 0.6496\n",
      "Epoch 20/20\n",
      "33/33 - 0s - loss: 0.1509 - accuracy: 0.9838 - val_loss: 0.7657 - val_accuracy: 0.6496\n"
     ]
    }
   ],
   "source": [
    "HYP_embedding_dim = 32\n",
    "\n",
    "HYP_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(HYP_vocab_size, HYP_embedding_dim, \n",
    "                                  input_length = max_length, name = 'HYPembed'),\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        tf.keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.001), activation = 'relu'),\n",
    "        tf.keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.001), activation = 'relu'),\n",
    "        tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "    ])\n",
    "\n",
    "HYP_model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "HYP_model.summary()\n",
    "HYP_n_epochs = 20\n",
    "\n",
    "HYP_history = HYP_model.fit(HYP_train_padded, blHYP_train, epochs = HYP_n_epochs, \n",
    "                            validation_data = (HYP_test_padded, blHYP_test), verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rhetorical NN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "RQembed (Embedding)          (None, 200, 3)            26796     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_17  (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 27,149\n",
      "Trainable params: 27,149\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "32/32 - 0s - loss: 0.7118 - accuracy: 0.4966 - val_loss: 0.7097 - val_accuracy: 0.4934\n",
      "Epoch 2/50\n",
      "32/32 - 0s - loss: 0.7080 - accuracy: 0.5171 - val_loss: 0.7065 - val_accuracy: 0.4934\n",
      "Epoch 3/50\n",
      "32/32 - 0s - loss: 0.7049 - accuracy: 0.5044 - val_loss: 0.7037 - val_accuracy: 0.4934\n",
      "Epoch 4/50\n",
      "32/32 - 0s - loss: 0.7023 - accuracy: 0.5054 - val_loss: 0.7015 - val_accuracy: 0.4934\n",
      "Epoch 5/50\n",
      "32/32 - 0s - loss: 0.7000 - accuracy: 0.5044 - val_loss: 0.6995 - val_accuracy: 0.5081\n",
      "Epoch 6/50\n",
      "32/32 - 0s - loss: 0.6983 - accuracy: 0.5495 - val_loss: 0.6979 - val_accuracy: 0.5448\n",
      "Epoch 7/50\n",
      "32/32 - 0s - loss: 0.6956 - accuracy: 0.5338 - val_loss: 0.6965 - val_accuracy: 0.5081\n",
      "Epoch 8/50\n",
      "32/32 - 0s - loss: 0.6931 - accuracy: 0.5779 - val_loss: 0.6944 - val_accuracy: 0.5521\n",
      "Epoch 9/50\n",
      "32/32 - 0s - loss: 0.6884 - accuracy: 0.6239 - val_loss: 0.6927 - val_accuracy: 0.5301\n",
      "Epoch 10/50\n",
      "32/32 - 0s - loss: 0.6820 - accuracy: 0.6141 - val_loss: 0.6885 - val_accuracy: 0.5991\n",
      "Epoch 11/50\n",
      "32/32 - 0s - loss: 0.6712 - accuracy: 0.6484 - val_loss: 0.6831 - val_accuracy: 0.6109\n",
      "Epoch 12/50\n",
      "32/32 - 0s - loss: 0.6523 - accuracy: 0.6934 - val_loss: 0.6758 - val_accuracy: 0.6197\n",
      "Epoch 13/50\n",
      "32/32 - 0s - loss: 0.6224 - accuracy: 0.7640 - val_loss: 0.6665 - val_accuracy: 0.6388\n",
      "Epoch 14/50\n",
      "32/32 - 0s - loss: 0.5780 - accuracy: 0.8168 - val_loss: 0.6553 - val_accuracy: 0.6505\n",
      "Epoch 15/50\n",
      "32/32 - 0s - loss: 0.5211 - accuracy: 0.8688 - val_loss: 0.6410 - val_accuracy: 0.6858\n",
      "Epoch 16/50\n",
      "32/32 - 0s - loss: 0.4557 - accuracy: 0.9099 - val_loss: 0.6335 - val_accuracy: 0.6858\n",
      "Epoch 17/50\n",
      "32/32 - 0s - loss: 0.3941 - accuracy: 0.9158 - val_loss: 0.6260 - val_accuracy: 0.6740\n",
      "Epoch 18/50\n",
      "32/32 - 0s - loss: 0.3410 - accuracy: 0.9324 - val_loss: 0.6332 - val_accuracy: 0.6740\n",
      "Epoch 19/50\n",
      "32/32 - 0s - loss: 0.3007 - accuracy: 0.9403 - val_loss: 0.6368 - val_accuracy: 0.6814\n",
      "Epoch 20/50\n",
      "32/32 - 0s - loss: 0.2580 - accuracy: 0.9579 - val_loss: 0.6404 - val_accuracy: 0.6872\n",
      "Epoch 21/50\n",
      "32/32 - 0s - loss: 0.2300 - accuracy: 0.9726 - val_loss: 0.6540 - val_accuracy: 0.6799\n",
      "Epoch 22/50\n",
      "32/32 - 0s - loss: 0.2100 - accuracy: 0.9716 - val_loss: 0.6615 - val_accuracy: 0.6872\n",
      "Epoch 23/50\n",
      "32/32 - 0s - loss: 0.1863 - accuracy: 0.9814 - val_loss: 0.6727 - val_accuracy: 0.6843\n",
      "Epoch 24/50\n",
      "32/32 - 0s - loss: 0.1715 - accuracy: 0.9892 - val_loss: 0.6855 - val_accuracy: 0.6916\n",
      "Epoch 25/50\n",
      "32/32 - 0s - loss: 0.1608 - accuracy: 0.9892 - val_loss: 0.6951 - val_accuracy: 0.6858\n",
      "Epoch 26/50\n",
      "32/32 - 0s - loss: 0.1528 - accuracy: 0.9853 - val_loss: 0.7073 - val_accuracy: 0.6887\n",
      "Epoch 27/50\n",
      "32/32 - 0s - loss: 0.1403 - accuracy: 0.9931 - val_loss: 0.7161 - val_accuracy: 0.6858\n",
      "Epoch 28/50\n",
      "32/32 - 0s - loss: 0.1340 - accuracy: 0.9931 - val_loss: 0.7263 - val_accuracy: 0.6828\n",
      "Epoch 29/50\n",
      "32/32 - 0s - loss: 0.1295 - accuracy: 0.9951 - val_loss: 0.7357 - val_accuracy: 0.6814\n",
      "Epoch 30/50\n",
      "32/32 - 0s - loss: 0.1228 - accuracy: 0.9941 - val_loss: 0.7430 - val_accuracy: 0.6799\n",
      "Epoch 31/50\n",
      "32/32 - 0s - loss: 0.1182 - accuracy: 0.9961 - val_loss: 0.7528 - val_accuracy: 0.6902\n",
      "Epoch 32/50\n",
      "32/32 - 0s - loss: 0.1164 - accuracy: 0.9951 - val_loss: 0.7594 - val_accuracy: 0.6887\n",
      "Epoch 33/50\n",
      "32/32 - 0s - loss: 0.1124 - accuracy: 0.9971 - val_loss: 0.7653 - val_accuracy: 0.6769\n",
      "Epoch 34/50\n",
      "32/32 - 0s - loss: 0.1087 - accuracy: 0.9990 - val_loss: 0.7700 - val_accuracy: 0.6858\n",
      "Epoch 35/50\n",
      "32/32 - 0s - loss: 0.1071 - accuracy: 0.9980 - val_loss: 0.7787 - val_accuracy: 0.6755\n",
      "Epoch 36/50\n",
      "32/32 - 0s - loss: 0.1050 - accuracy: 0.9980 - val_loss: 0.7809 - val_accuracy: 0.6784\n",
      "Epoch 37/50\n",
      "32/32 - 0s - loss: 0.1014 - accuracy: 1.0000 - val_loss: 0.7861 - val_accuracy: 0.6784\n",
      "Epoch 38/50\n",
      "32/32 - 0s - loss: 0.1001 - accuracy: 1.0000 - val_loss: 0.7900 - val_accuracy: 0.6799\n",
      "Epoch 39/50\n",
      "32/32 - 0s - loss: 0.0980 - accuracy: 1.0000 - val_loss: 0.7937 - val_accuracy: 0.6755\n",
      "Epoch 40/50\n",
      "32/32 - 0s - loss: 0.0964 - accuracy: 1.0000 - val_loss: 0.7969 - val_accuracy: 0.6784\n",
      "Epoch 41/50\n",
      "32/32 - 0s - loss: 0.0948 - accuracy: 1.0000 - val_loss: 0.8043 - val_accuracy: 0.6887\n",
      "Epoch 42/50\n",
      "32/32 - 0s - loss: 0.0933 - accuracy: 1.0000 - val_loss: 0.8061 - val_accuracy: 0.6799\n",
      "Epoch 43/50\n",
      "32/32 - 0s - loss: 0.0928 - accuracy: 0.9990 - val_loss: 0.8173 - val_accuracy: 0.6858\n",
      "Epoch 44/50\n",
      "32/32 - 0s - loss: 0.0908 - accuracy: 1.0000 - val_loss: 0.8080 - val_accuracy: 0.6799\n",
      "Epoch 45/50\n",
      "32/32 - 0s - loss: 0.0897 - accuracy: 1.0000 - val_loss: 0.8120 - val_accuracy: 0.6799\n",
      "Epoch 46/50\n",
      "32/32 - 0s - loss: 0.0880 - accuracy: 1.0000 - val_loss: 0.8125 - val_accuracy: 0.6769\n",
      "Epoch 47/50\n",
      "32/32 - 0s - loss: 0.0866 - accuracy: 1.0000 - val_loss: 0.8178 - val_accuracy: 0.6769\n",
      "Epoch 48/50\n",
      "32/32 - 0s - loss: 0.0866 - accuracy: 1.0000 - val_loss: 0.8143 - val_accuracy: 0.6858\n",
      "Epoch 49/50\n",
      "32/32 - 0s - loss: 0.0845 - accuracy: 1.0000 - val_loss: 0.8157 - val_accuracy: 0.6843\n",
      "Epoch 50/50\n",
      "32/32 - 0s - loss: 0.0836 - accuracy: 1.0000 - val_loss: 0.8174 - val_accuracy: 0.6814\n"
     ]
    }
   ],
   "source": [
    "RQ_embedding_dim = 3\n",
    "\n",
    "RQ_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(RQ_vocab_size, RQ_embedding_dim, input_length = max_length, name = 'RQembed'),\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        tf.keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.001), activation = 'relu'),\n",
    "        tf.keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.001), activation = 'relu'), \n",
    "        tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "    ])\n",
    "\n",
    "RQ_model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "RQ_model.summary()\n",
    "RQ_n_epochs = 50\n",
    "\n",
    "RQ_history = RQ_model.fit(RQ_train_padded, blRQ_train, epochs = RQ_n_epochs, \n",
    "                          validation_data = (RQ_test_padded, blRQ_test), verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./logs/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0023s vs `on_train_batch_end` time: 0.0059s). Check your callbacks.\n",
      "32/32 - 0s - loss: 0.0828 - accuracy: 1.0000 - val_loss: 0.8192 - val_accuracy: 0.6843\n",
      "Epoch 2/50\n",
      "32/32 - 0s - loss: 0.0819 - accuracy: 1.0000 - val_loss: 0.8202 - val_accuracy: 0.6872\n",
      "Epoch 3/50\n",
      "32/32 - 0s - loss: 0.0810 - accuracy: 1.0000 - val_loss: 0.8245 - val_accuracy: 0.6769\n",
      "Epoch 4/50\n",
      "32/32 - 0s - loss: 0.0795 - accuracy: 1.0000 - val_loss: 0.8225 - val_accuracy: 0.6799\n",
      "Epoch 5/50\n",
      "32/32 - 0s - loss: 0.0790 - accuracy: 1.0000 - val_loss: 0.8239 - val_accuracy: 0.6725\n",
      "Epoch 6/50\n",
      "32/32 - 0s - loss: 0.0780 - accuracy: 1.0000 - val_loss: 0.8231 - val_accuracy: 0.6784\n",
      "Epoch 7/50\n",
      "32/32 - 0s - loss: 0.0770 - accuracy: 1.0000 - val_loss: 0.8237 - val_accuracy: 0.6784\n",
      "Epoch 8/50\n",
      "32/32 - 0s - loss: 0.0764 - accuracy: 1.0000 - val_loss: 0.8227 - val_accuracy: 0.6784\n",
      "Epoch 9/50\n",
      "32/32 - 0s - loss: 0.0754 - accuracy: 1.0000 - val_loss: 0.8220 - val_accuracy: 0.6872\n",
      "Epoch 10/50\n",
      "32/32 - 0s - loss: 0.0746 - accuracy: 1.0000 - val_loss: 0.8320 - val_accuracy: 0.6740\n",
      "Epoch 11/50\n",
      "32/32 - 0s - loss: 0.0738 - accuracy: 1.0000 - val_loss: 0.8226 - val_accuracy: 0.6843\n",
      "Epoch 12/50\n",
      "32/32 - 0s - loss: 0.0736 - accuracy: 1.0000 - val_loss: 0.8239 - val_accuracy: 0.6799\n",
      "Epoch 13/50\n",
      "32/32 - 0s - loss: 0.0724 - accuracy: 1.0000 - val_loss: 0.8241 - val_accuracy: 0.6769\n",
      "Epoch 14/50\n",
      "32/32 - 0s - loss: 0.0716 - accuracy: 1.0000 - val_loss: 0.8295 - val_accuracy: 0.6755\n",
      "Epoch 15/50\n",
      "32/32 - 0s - loss: 0.0709 - accuracy: 1.0000 - val_loss: 0.8245 - val_accuracy: 0.6755\n",
      "Epoch 16/50\n",
      "32/32 - 0s - loss: 0.0701 - accuracy: 1.0000 - val_loss: 0.8243 - val_accuracy: 0.6814\n",
      "Epoch 17/50\n",
      "32/32 - 0s - loss: 0.0696 - accuracy: 1.0000 - val_loss: 0.8240 - val_accuracy: 0.6769\n",
      "Epoch 18/50\n",
      "32/32 - 0s - loss: 0.0691 - accuracy: 1.0000 - val_loss: 0.8246 - val_accuracy: 0.6843\n",
      "Epoch 19/50\n",
      "32/32 - 0s - loss: 0.0681 - accuracy: 1.0000 - val_loss: 0.8249 - val_accuracy: 0.6828\n",
      "Epoch 20/50\n",
      "32/32 - 0s - loss: 0.0673 - accuracy: 1.0000 - val_loss: 0.8291 - val_accuracy: 0.6711\n",
      "Epoch 21/50\n",
      "32/32 - 0s - loss: 0.0668 - accuracy: 1.0000 - val_loss: 0.8284 - val_accuracy: 0.6681\n",
      "Epoch 22/50\n",
      "32/32 - 0s - loss: 0.0661 - accuracy: 1.0000 - val_loss: 0.8253 - val_accuracy: 0.6784\n",
      "Epoch 23/50\n",
      "32/32 - 0s - loss: 0.0654 - accuracy: 1.0000 - val_loss: 0.8257 - val_accuracy: 0.6828\n",
      "Epoch 24/50\n",
      "32/32 - 0s - loss: 0.0648 - accuracy: 1.0000 - val_loss: 0.8311 - val_accuracy: 0.6740\n",
      "Epoch 25/50\n",
      "32/32 - 0s - loss: 0.0640 - accuracy: 1.0000 - val_loss: 0.8270 - val_accuracy: 0.6769\n",
      "Epoch 26/50\n",
      "32/32 - 0s - loss: 0.0635 - accuracy: 1.0000 - val_loss: 0.8284 - val_accuracy: 0.6740\n",
      "Epoch 27/50\n",
      "32/32 - 0s - loss: 0.0631 - accuracy: 1.0000 - val_loss: 0.8324 - val_accuracy: 0.6711\n",
      "Epoch 28/50\n",
      "32/32 - 0s - loss: 0.0623 - accuracy: 1.0000 - val_loss: 0.8269 - val_accuracy: 0.6784\n",
      "Epoch 29/50\n",
      "32/32 - 0s - loss: 0.0616 - accuracy: 1.0000 - val_loss: 0.8278 - val_accuracy: 0.6814\n",
      "Epoch 30/50\n",
      "32/32 - 0s - loss: 0.0611 - accuracy: 1.0000 - val_loss: 0.8282 - val_accuracy: 0.6769\n",
      "Epoch 31/50\n",
      "32/32 - 0s - loss: 0.0605 - accuracy: 1.0000 - val_loss: 0.8274 - val_accuracy: 0.6799\n",
      "Epoch 32/50\n",
      "32/32 - 0s - loss: 0.0600 - accuracy: 1.0000 - val_loss: 0.8279 - val_accuracy: 0.6814\n",
      "Epoch 33/50\n",
      "32/32 - 0s - loss: 0.0594 - accuracy: 1.0000 - val_loss: 0.8286 - val_accuracy: 0.6799\n",
      "Epoch 34/50\n",
      "32/32 - 0s - loss: 0.0590 - accuracy: 1.0000 - val_loss: 0.8306 - val_accuracy: 0.6784\n",
      "Epoch 35/50\n",
      "32/32 - 0s - loss: 0.0585 - accuracy: 1.0000 - val_loss: 0.8330 - val_accuracy: 0.6725\n",
      "Epoch 36/50\n",
      "32/32 - 0s - loss: 0.0578 - accuracy: 1.0000 - val_loss: 0.8297 - val_accuracy: 0.6784\n",
      "Epoch 37/50\n",
      "32/32 - 0s - loss: 0.0573 - accuracy: 1.0000 - val_loss: 0.8291 - val_accuracy: 0.6784\n",
      "Epoch 38/50\n",
      "32/32 - 0s - loss: 0.0568 - accuracy: 1.0000 - val_loss: 0.8329 - val_accuracy: 0.6740\n",
      "Epoch 39/50\n",
      "32/32 - 0s - loss: 0.0563 - accuracy: 1.0000 - val_loss: 0.8305 - val_accuracy: 0.6799\n",
      "Epoch 40/50\n",
      "32/32 - 0s - loss: 0.0557 - accuracy: 1.0000 - val_loss: 0.8314 - val_accuracy: 0.6755\n",
      "Epoch 41/50\n",
      "32/32 - 0s - loss: 0.0555 - accuracy: 1.0000 - val_loss: 0.8309 - val_accuracy: 0.6755\n",
      "Epoch 42/50\n",
      "32/32 - 0s - loss: 0.0548 - accuracy: 1.0000 - val_loss: 0.8313 - val_accuracy: 0.6799\n",
      "Epoch 43/50\n",
      "32/32 - 0s - loss: 0.0542 - accuracy: 1.0000 - val_loss: 0.8336 - val_accuracy: 0.6755\n",
      "Epoch 44/50\n",
      "32/32 - 0s - loss: 0.0538 - accuracy: 1.0000 - val_loss: 0.8345 - val_accuracy: 0.6755\n",
      "Epoch 45/50\n",
      "32/32 - 0s - loss: 0.0535 - accuracy: 1.0000 - val_loss: 0.8332 - val_accuracy: 0.6799\n",
      "Epoch 46/50\n",
      "32/32 - 0s - loss: 0.0528 - accuracy: 1.0000 - val_loss: 0.8394 - val_accuracy: 0.6725\n",
      "Epoch 47/50\n",
      "32/32 - 0s - loss: 0.0526 - accuracy: 1.0000 - val_loss: 0.8330 - val_accuracy: 0.6755\n",
      "Epoch 48/50\n",
      "32/32 - 0s - loss: 0.0519 - accuracy: 1.0000 - val_loss: 0.8347 - val_accuracy: 0.6740\n",
      "Epoch 49/50\n",
      "32/32 - 0s - loss: 0.0515 - accuracy: 1.0000 - val_loss: 0.8355 - val_accuracy: 0.6725\n",
      "Epoch 50/50\n",
      "32/32 - 0s - loss: 0.0510 - accuracy: 1.0000 - val_loss: 0.8350 - val_accuracy: 0.6755\n"
     ]
    }
   ],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "RQ_history = RQ_model.fit(RQ_train_padded, blRQ_train, epochs = 50, \n",
    "                            validation_data = (RQ_test_padded, blRQ_test), verbose = 2, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 27606), started 0:20:15 ago. (Use '!kill 27606' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-a3b0c6e609151abf\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-a3b0c6e609151abf\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_classifier(txt):\n",
    "    text = clean_text(txt)\n",
    "    input_sequence = Type_tok.texts_to_sequences([text])\n",
    "    input_padded = pad_sequences(input_sequence, maxlen=max_length, padding = 'post', truncating = 'post')\n",
    "    \n",
    "    if text == \"\": return\n",
    "    \n",
    "    prediction_list = Type_model.predict(input_padded)\n",
    "    prediction_location = np.argmax(prediction_list)\n",
    "    \n",
    "    type_list = ['general', 'hyperbole', 'rhetorical']\n",
    "\n",
    "    return(type_list[prediction_location])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'general'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type_classifier(\"That man is as tall as a house.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gthreshold = 0.7\n",
    "Hthreshold = 0.7\n",
    "Rthreshold = 0.7\n",
    "\n",
    "def sarcasm_classifier(txt, Type):\n",
    "    text = clean_text(txt)\n",
    "    if(Type == None):\n",
    "        print(\"No input text was found\")\n",
    "        return\n",
    "        \n",
    "    elif(Type == 'general'):\n",
    "        input_sequence = GENtok.texts_to_sequences([text])\n",
    "        input_padded = pad_sequences(input_sequence, maxlen=max_length,\n",
    "                                     padding = 'post', truncating = 'post')\n",
    "\n",
    "        predicted_probability = GEN_model.predict(input_padded)\n",
    "    \n",
    "        if(predicted_probability <= Gthreshold):\n",
    "            prediction = \"not sarcastic\"\n",
    "        elif(predicted_probability > Gthreshold):\n",
    "            prediction = \"sarcastic\"\n",
    "        \n",
    "    elif(Type == 'hyperbole'):\n",
    "        input_sequence = HYPtok.texts_to_sequences([text])\n",
    "        input_padded = pad_sequences(input_sequence, maxlen=max_length, padding = 'post', truncating = 'post')\n",
    "\n",
    "        predicted_probability = HYP_model.predict(input_padded)\n",
    "    \n",
    "\n",
    "        if(predicted_probability <= Hthreshold):\n",
    "            prediction = \"not sarcastic\"\n",
    "        elif(predicted_probability > Hthreshold):\n",
    "            prediction = \"sarcastic\"\n",
    "            \n",
    "    elif(Type == 'rhetorical'):\n",
    "        input_sequence = RQtok.texts_to_sequences([text])\n",
    "        input_padded = pad_sequences(input_sequence, maxlen=max_length, padding = 'post', truncating = 'post')\n",
    "\n",
    "        predicted_probability = RQ_model.predict(input_padded)\n",
    "    \n",
    "        if(predicted_probability <= Rthreshold):\n",
    "            prediction = \"not sarcastic\"\n",
    "        elif(predicted_probability > Rthreshold):\n",
    "            prediction = \"sarcastic\"\n",
    "            \n",
    "    print(\"INPUT: \" + text)\n",
    "    print(\"Type: \" + Type + \" statement\")\n",
    "    print(\"prediction: \" + prediction)\n",
    "    print(str(predicted_probability[0][0]) + \" activation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kappa(text):\n",
    "    return sarcasm_classifier(text, type_classifier(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: hello my name is jason\n",
      "Type: general statement\n",
      "prediction: sarcastic\n",
      "0.99082863 activation\n"
     ]
    }
   ],
   "source": [
    "Kappa(\"Hello my name is Jason\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update1(df, word, column):\n",
    "    loc = df.loc[df['Words']==word]\n",
    "    locI = int(loc.index.tolist()[0])\n",
    "    new_value = int(loc[column]) + 1\n",
    "    new_v_series = pd.Series([new_value], name=column, index=[locI])\n",
    "    df.update(new_v_series)\n",
    "    return df\n",
    "\n",
    "def binDecode(x, thres):\n",
    "    if (x < thres):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def Count_Correct(model, test_data, test_padded, test_label, word_index, thres):\n",
    "    predicted_labels = model.predict(test_padded)\n",
    "    words = list(word_index.keys())\n",
    "    \n",
    "    df = pd.DataFrame(columns = ['Words', 'Correct', 'Total', 'Rightly Sarcastic'])\n",
    "    df['Words'] = words\n",
    "    df['Correct'] = [0]*len(word_index)\n",
    "    df['Total'] = [0]*len(word_index)\n",
    "    df['Rightly Sarcastic'] = [0]*len(word_index)\n",
    "    df['Incorrectly Sarcastic'] = [0]*len(word_index)\n",
    "    \n",
    "    for i in range(len(test_data)):\n",
    "        boo = (binDecode(predicted_labels[i], thres) == binDecode(test_label[i], thres))\n",
    "        element = test_data[i]\n",
    "        for word in element.split():\n",
    "            if (word in (df.Words.values)):\n",
    "                word = word\n",
    "            else: word = \"<OOV>\"\n",
    "            df = update1(df, word, 'Total')\n",
    "            if boo:\n",
    "                df = update1(df, word, 'Correct')\n",
    "                if (binDecode(predicted_labels[i], thres) == 1):\n",
    "                    df = update1(df, word, 'Rightly Sarcastic')\n",
    "            else:\n",
    "                if (binDecode(predicted_labels[i], thres) == 1):\n",
    "                    df = update1(df, word, 'Incorrectly Sarcastic')\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENdf = Count_Correct(GEN_model, GEN_test, GEN_test_padded, blGEN_test, GEN_word_index, Gthreshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPdf = Count_Correct(HYP_model, HYP_test, HYP_test_padded, blHYP_test, HYP_word_index, Hthreshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "RQdf = Count_Correct(RQ_model, RQ_test, RQ_test_padded, blRQ_test, RQ_word_index, Rthreshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chdir(cwd)\n",
    "if (os.path.exists(cwd+\"/ModelsSARC\")):\n",
    "    print(\"File exists\")\n",
    "else:\n",
    "    makedirs(\"ModelsSARC\")\n",
    "    chdir(cwd + \"/ModelsSARC\")\n",
    "\n",
    "    Type_model.save('Type_model.h5')\n",
    "    GEN_model.save('GEN_model.h5')\n",
    "    HYP_model.save('HYP_model.h5')\n",
    "    RQ_model.save('RQ_model.h5')\n",
    "\n",
    "    with open('Type_tok.pickle', 'wb') as handle:\n",
    "        pickle.dump(GENtok, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('GENtok.pickle', 'wb') as handle:\n",
    "        pickle.dump(GENtok, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('HYPtok.pickle', 'wb') as handle:\n",
    "        pickle.dump(HYPtok, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('RQtok.pickle', 'wb') as handle:\n",
    "        pickle.dump(RQtok, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "chdir(cwd)\n",
    "if (os.path.exists(cwd+\"/dfSARC\")):\n",
    "    print(\"File exists\")\n",
    "else:\n",
    "    makedirs(\"dfSARC\")\n",
    "    chdir(cwd + \"/dfSARC\")\n",
    "    \n",
    "    GENdf.to_csv(\"GENdf.csv\")\n",
    "    HYPdf.to_csv(\"HYPdf.csv\")\n",
    "    RQdf.to_csv(\"RQdf.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENweights = GEN_model.get_layer('GENembed').get_weights()[0]\n",
    "HYPweights = HYP_model.get_layer('HYPembed').get_weights()[0]\n",
    "RQweights = RQ_model.get_layer('RQembed').get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEN_vocab = GEN_word_index\n",
    "Gout_v = io.open('Gvecs.tsv', 'w', encoding='utf-8')\n",
    "Gout_m = io.open('Gmeta.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for num, word in enumerate(GEN_vocab):\n",
    "    if num == 0: continue\n",
    "    vec = GENweights[num]\n",
    "    Gout_m.write(word + \"\\n\")\n",
    "    Gout_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "Gout_v.close()\n",
    "Gout_m.close()\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "except ImportError:\n",
    "    pass\n",
    "else:\n",
    "    files.download('Gvecs.tsv')\n",
    "    files.download('Gmeta.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "HYP_vocab = HYP_word_index\n",
    "Hout_v = io.open('Hvecs.tsv', 'w', encoding='utf-8')\n",
    "Hout_m = io.open('Hmeta.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for num, word in enumerate(HYP_vocab):\n",
    "    if num == 0: continue\n",
    "    vec = HYPweights[num]\n",
    "    Hout_m.write(word + \"\\n\")\n",
    "    Hout_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "Hout_v.close()\n",
    "Hout_m.close()\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "except ImportError:\n",
    "    pass\n",
    "else:\n",
    "    files.download('Hvecs.tsv')\n",
    "    files.download('Hmeta.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "RQ_vocab = RQ_word_index\n",
    "Rout_v = io.open('Rvecs.tsv', 'w', encoding='utf-8')\n",
    "Rout_m = io.open('Rmeta.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for num, word in enumerate(RQ_vocab):\n",
    "    if num == 0: continue\n",
    "    vec = RQweights[num]\n",
    "    Rout_m.write(word + \"\\n\")\n",
    "    Rout_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "Rout_v.close()\n",
    "Rout_m.close()\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "except ImportError:\n",
    "    pass\n",
    "else:\n",
    "    files.download('Rvecs.tsv')\n",
    "    files.download('Rmeta.tsv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
